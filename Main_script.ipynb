{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Source Medium articles, Youtube, Coursera, most important Stackoverflow'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Source Medium articles, Youtube, Coursera, most important Stackoverflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training_data = os.getcwd()+'/training_data.tsv'\n",
    "path_test_data=os.getcwd()+'/evalution_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regex model\n",
    "\n",
    "data = pd.read_csv(path_training_data, header=0,delimiter=\"\\t\",quoting=3)\n",
    "\n",
    "list_sent= list(data['sent'])\n",
    "list_labels = list(data['label'])\n",
    "\n",
    "mega_list_label=[]\n",
    "for idx,label in enumerate(list_labels):\n",
    "    if label=='Not Found':\n",
    "        pass\n",
    "    else:\n",
    "        mega_list_label.append([list_sent[idx],label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern with words on both side \n",
    "\n",
    "some_list=[]\n",
    "for i in mega_list_label:\n",
    "    name = i[1]\n",
    "    name = re.sub('[^A-Za-z0-9]+', ' ', name)\n",
    "    sub = '(\\w*)\\W*('+name+')\\W*(\\w*)'\n",
    "    str1 = i[0]\n",
    "    for i in re.findall(sub, str1, re.I):\n",
    "        some_list.append ([\" \".join([x for x in i if x != \"\"]),name])\n",
    "\n",
    "some_pattern_list=[]\n",
    "for double_list in some_list:\n",
    "    if double_list[1] in double_list[0]:\n",
    "        some_pattern_list.append(double_list[0].replace(double_list[1], \" \"))\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "pattern_list1=((Counter(some_pattern_list)).most_common())\n",
    "pattern_list=[tup[0] for tup in pattern_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns with two words on the left \n",
    "some_new_list=[]\n",
    "for i in mega_list_label:\n",
    "    name = i[1]\n",
    "    name = re.sub('[^A-Za-z0-9]+', ' ', name)\n",
    "    sub = '(\\w*)\\W*(\\w*)\\W*('+name+')'\n",
    "    str1 = i[0]\n",
    "    for i in re.findall(sub, str1, re.I):\n",
    "        some_new_list.append ([\" \".join([x for x in i if x != \"\"]),name])  \n",
    "some_pattern_list_new=[]\n",
    "for double_list in some_new_list:\n",
    "    if double_list[1] in double_list[0]:\n",
    "        some_pattern_list_new.append(double_list[0].replace(double_list[1], \" \"))\n",
    "    else:\n",
    "        pass\n",
    "pattern_list2=((Counter(some_pattern_list_new)).most_common())\n",
    "pattern_list_2=[tup[0] for tup in pattern_list2]\n",
    "len_2_words=int((len(pattern_list_2))*.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ######## Creating the regex_matcher.py file ########\n",
    "\n",
    "    target = io.open('regex_matcher.py','a', encoding=\"utf-8\")\n",
    "    target.truncate()\n",
    "    target.write('import re')\n",
    "    target.write('\\n')\n",
    "    target.write('def main_regex_matcher(path_test):')\n",
    "    target.write('\\n')\n",
    "    target.write(\n",
    "        '''\n",
    "    master_list=[]\n",
    "    sent_list=[]\n",
    "    data = open(path_test,'r')\n",
    "    for text in data:\n",
    "    found = 0\n",
    "    small_master_list=[]\n",
    "        '''\n",
    "    )    \n",
    "    target.write('\\n')\n",
    "    target.close()\n",
    "    for pat in pattern_list:\n",
    "        boundaries=pat.split()\n",
    "        if len(boundaries)==2:\n",
    "            target = io.open('regex_matcher.py','a', encoding=\"utf-8\")\n",
    "            target.write('        m = re.search('+\"' \"+boundaries[0]+' '+'(.+?)'+' '+boundaries[1]+\" '\"+', text)')\n",
    "            target.write('\\n')\n",
    "            target.write('        if m:')\n",
    "            target.write('\\n')\n",
    "            target.write('            found = m.group(1)')\n",
    "            target.write('\\n')\n",
    "            target.write('            small_master_list.append(found)')\n",
    "            target.write('\\n')\n",
    "            target.close()\n",
    "        else:\n",
    "            pass\n",
    "    for pat in pattern_list_2[:len_2_words]:\n",
    "        boundaries=pat.split()\n",
    "        if len(boundaries)==2:\n",
    "            target = io.open('regex_matcher.py','a', encoding=\"utf-8\")\n",
    "            target.write('        m = re.search('+\"' \"+boundaries[0]+' '+boundaries[1]+' (.*)'+\"'\"+', text)')\n",
    "            target.write('\\n')\n",
    "            target.write('        if m:')\n",
    "            target.write('\\n')\n",
    "            target.write('            found = m.group(1)')\n",
    "            target.write('\\n')\n",
    "            target.write('            small_master_list.append(found)')\n",
    "            target.write('\\n')\n",
    "            target.close()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    target = open('regex_matcher.py', 'a')\n",
    "    target.write(\n",
    "        '''\n",
    "        if found==0 : # if none of the patterns match give it as Not found\n",
    "            small_master_list.append('Not Found')\n",
    "        master_list.append(small_master_list)\n",
    "\n",
    "    no_no_words=['on','for','to','at','by'] # list of words which if occured in the output will be penalised while giving a score\n",
    "    final_output=[]\n",
    "    ############ selecting one from the options\n",
    "    for options in master_list:\n",
    "        if len(options)==1:\n",
    "            final_output.append(options[0])#if only one pattern extracted use it \n",
    "        else:\n",
    "            sent_score_list=[] # else score all the options to select the best one\n",
    "            for option in options:\n",
    "                l=option.split()\n",
    "                score = 0\n",
    "                score = len(l)\n",
    "                for word in l:\n",
    "                    if word in no_no_words:\n",
    "                        score = score -3 #penalise the no_no_words\n",
    "                    else:\n",
    "                        pass\n",
    "                sent_score_list.append(score)\n",
    "            m = max(sent_score_list)\n",
    "            indx=[i for i, j in enumerate(sent_score_list) if j == m] # returns a list of all the index which have max score\n",
    "            index = indx[-1]#pick the last element as an index \n",
    "            final_output.append(options[index])\n",
    "\n",
    "    tups=zip(sent_list, final_output)\n",
    "    final_list = [list(l) for l in tups]\n",
    "\n",
    "    for small_list in final_list:\n",
    "        target = open(\"output.txt\", \"a\")\n",
    "        target.write('\\\\n')\n",
    "        target.write(' '.join(small_list[0].split())+'\\\\t'+small_list[1])\n",
    "        target.write('\\\\n')\n",
    "        target.close()\n",
    "    return(final_output)\n",
    "    ''')\n",
    "    target.write('\\n')\n",
    "    target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "inside_extration....This might take a while\nextracted\n"
    }
   ],
   "source": [
    "import regex_matcher\n",
    "\n",
    "print('inside_extration....This might take a while')\n",
    "extractions_from_regex=regex_matcher.main_regex_matcher(path_test_data)\n",
    "print('extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the output in the file \n",
    "with io.open('output.txt','w+',encoding=\"utf-8\") as f:\n",
    "    f.truncate()\n",
    "    for item in extractions_from_regex:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cleaning data\n\n"
    }
   ],
   "source": [
    "## label classifier\n",
    "\n",
    "data = pd.read_csv(('training_data.tsv'), header=0,delimiter=\"\\t\",quoting=3)\n",
    "data.dropna(inplace=True) #drop NaN value\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.loc[data['label'] != 'Not Found' , 'label'] = 'Found' #to change data in only found and not found\n",
    "\n",
    "X = data['sent']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=0)\n",
    "\n",
    "clean_train_data = []\n",
    "print(\"cleaning data\\n\")\n",
    "for i in (X_train):\n",
    "    clean_train_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words   \n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "np.asarray(train_data_features)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(train_data_features)\n",
    "\n",
    "clf = svm.LinearSVC(loss='hinge').fit(X_train_tfidf,y_train  )\n",
    "\n",
    "f = open('my_classifier.pickle', 'wb')\n",
    "pickle.dump(clf, f)\n",
    "f.close()\n",
    "f = open('my_vectorizer.pickle', 'wb')\n",
    "pickle.dump(vectorizer, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "master_data=open(path_test_data,'r')\n",
    "for sent in master_data:\n",
    "    data.append(' '.join(sent.split()))\n",
    "df=pd.DataFrame(data)\n",
    "df=df[0]\n",
    "\n",
    "clean_test_data = []\n",
    "for i in df:\n",
    "    clean_test_data.append(i)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "np.asarray(test_data_features)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(test_data_features)\n",
    "\n",
    "result = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=[]\n",
    "for idx,prediction in enumerate(result):\n",
    "    if (prediction=='Not Found'):\n",
    "        final_result.append('Not Found') \n",
    "    else:\n",
    "        final_result.append(extractions_from_regex[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = pd.DataFrame(data={\"sent\": df, \"label\": final_result})\n",
    "final_output.to_csv(('submission.csv'), index=False, quoting=3, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Done !!!!!!!!!!!'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Done !!!!!!!!!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}